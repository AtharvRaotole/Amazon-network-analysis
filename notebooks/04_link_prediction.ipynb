{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Link Prediction: Amazon Product Co-Purchasing Network\n",
        "\n",
        "This notebook implements and compares link prediction methods to predict missing links in the Amazon co-purchasing network.\n",
        "\n",
        "## Objectives:\n",
        "1. Load train graph and test edges (positive and negative)\n",
        "2. Implement similarity-based methods (Common Neighbors, Adamic-Adar, Jaccard)\n",
        "3. Evaluate each similarity method with metrics and visualizations\n",
        "4. Implement ML-based method (Random Forest) with feature extraction\n",
        "5. Compare all methods comprehensively\n",
        "6. Analyze results and provide business insights\n",
        "7. Save all results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup\n",
        "\n",
        "Import all necessary libraries for link prediction, evaluation, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src directory to path for imports\n",
        "if Path.cwd().name == 'notebooks':\n",
        "    project_root = Path.cwd().parent\n",
        "else:\n",
        "    project_root = Path.cwd()\n",
        "\n",
        "sys.path.insert(0, str(project_root / 'src'))\n",
        "os.chdir(project_root)\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Network analysis\n",
        "import networkx as nx\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Project modules\n",
        "from data_loader import load_saved_graph\n",
        "from link_prediction import (\n",
        "    common_neighbors_score,\n",
        "    adamic_adar_score,\n",
        "    jaccard_coefficient_score,\n",
        "    calculate_all_similarity_scores,\n",
        "    predict_links_similarity,\n",
        "    evaluate_link_prediction\n",
        ")\n",
        "from ml_link_prediction import (\n",
        "    extract_edge_features,\n",
        "    prepare_training_data,\n",
        "    train_random_forest,\n",
        "    predict_links_ml,\n",
        "    evaluate_ml_model,\n",
        "    get_feature_importance\n",
        ")\n",
        "from link_prediction_evaluation import (\n",
        "    plot_roc_curves,\n",
        "    plot_precision_recall_curves,\n",
        "    plot_confusion_matrices,\n",
        "    plot_feature_importance,\n",
        "    create_comparison_table,\n",
        "    plot_score_distributions,\n",
        "    generate_link_prediction_report\n",
        ")\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid' if 'seaborn-v0_8-darkgrid' in plt.style.available \n",
        "              else 'seaborn-darkgrid' if 'seaborn-darkgrid' in plt.style.available \n",
        "              else 'ggplot')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Train Graph and Test Edges\n",
        "\n",
        "Load the training graph and test edges (positive and negative) that were created during preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training graph\n",
        "train_graph_path = \"data/processed/splits/train_graph.pkl\"\n",
        "print(f\"Loading training graph from: {train_graph_path}\")\n",
        "\n",
        "with open(train_graph_path, 'rb') as f:\n",
        "    G_train = pickle.load(f)\n",
        "\n",
        "print(f\"\\nâœ… Training graph loaded!\")\n",
        "print(f\"   Nodes: {G_train.number_of_nodes():,}\")\n",
        "print(f\"   Edges: {G_train.number_of_edges():,}\")\n",
        "print(f\"   Type: {'Undirected' if not G_train.is_directed() else 'Directed'}\")\n",
        "\n",
        "# Load test edges\n",
        "pos_test_path = \"data/processed/splits/positive_test_edges.pkl\"\n",
        "neg_test_path = \"data/processed/splits/negative_test_edges.pkl\"\n",
        "\n",
        "print(f\"\\nLoading test edges...\")\n",
        "with open(pos_test_path, 'rb') as f:\n",
        "    pos_test_edges = pickle.load(f)\n",
        "\n",
        "with open(neg_test_path, 'rb') as f:\n",
        "    neg_test_edges = pickle.load(f)\n",
        "\n",
        "print(f\"âœ… Test edges loaded!\")\n",
        "print(f\"   Positive test edges: {len(pos_test_edges):,}\")\n",
        "print(f\"   Negative test edges: {len(neg_test_edges):,}\")\n",
        "print(f\"   Total test edges: {len(pos_test_edges) + len(neg_test_edges):,}\")\n",
        "\n",
        "# For faster computation, we'll use a sample of test edges\n",
        "# You can adjust this based on your computational resources\n",
        "SAMPLE_SIZE = min(5000, len(pos_test_edges))\n",
        "print(f\"\\nðŸ“Š Using sample of {SAMPLE_SIZE} positive and {SAMPLE_SIZE} negative edges for evaluation\")\n",
        "pos_test_sample = pos_test_edges[:SAMPLE_SIZE]\n",
        "neg_test_sample = neg_test_edges[:SAMPLE_SIZE]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Similarity-Based Link Prediction Methods\n",
        "\n",
        "Implement and evaluate three similarity-based methods: Common Neighbors, Adamic-Adar, and Jaccard Coefficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Common Neighbors\n",
        "\n",
        "Common Neighbors counts the number of shared neighbors between two nodes. Higher counts indicate higher likelihood of a link.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating Common Neighbors method...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "cn_metrics = evaluate_link_prediction(\n",
        "    G_train, \n",
        "    pos_test_sample, \n",
        "    neg_test_sample, \n",
        "    method='common_neighbors'\n",
        ")\n",
        "cn_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ… Common Neighbors evaluation complete!\")\n",
        "print(f\"   Execution time: {cn_time:.2f} seconds\")\n",
        "print(f\"   Precision: {cn_metrics['precision']:.4f}\")\n",
        "print(f\"   Recall: {cn_metrics['recall']:.4f}\")\n",
        "print(f\"   F1 Score: {cn_metrics['f1']:.4f}\")\n",
        "print(f\"   AUC-ROC: {cn_metrics['auc_roc']:.4f}\")\n",
        "print(f\"   AUC-PR: {cn_metrics['auc_pr']:.4f}\")\n",
        "print(f\"   Optimal Threshold: {cn_metrics['optimal_threshold']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Adamic-Adar\n",
        "\n",
        "Adamic-Adar weights common neighbors by the inverse logarithm of their degree. Nodes with fewer neighbors contribute more to the score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating Adamic-Adar method...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "aa_metrics = evaluate_link_prediction(\n",
        "    G_train, \n",
        "    pos_test_sample, \n",
        "    neg_test_sample, \n",
        "    method='adamic_adar'\n",
        ")\n",
        "aa_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ… Adamic-Adar evaluation complete!\")\n",
        "print(f\"   Execution time: {aa_time:.2f} seconds\")\n",
        "print(f\"   Precision: {aa_metrics['precision']:.4f}\")\n",
        "print(f\"   Recall: {aa_metrics['recall']:.4f}\")\n",
        "print(f\"   F1 Score: {aa_metrics['f1']:.4f}\")\n",
        "print(f\"   AUC-ROC: {aa_metrics['auc_roc']:.4f}\")\n",
        "print(f\"   AUC-PR: {aa_metrics['auc_pr']:.4f}\")\n",
        "print(f\"   Optimal Threshold: {aa_metrics['optimal_threshold']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Jaccard Coefficient\n",
        "\n",
        "Jaccard Coefficient is the ratio of common neighbors to total unique neighbors. Range: [0, 1] where 1 indicates all neighbors are shared.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating Jaccard Coefficient method...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "jc_metrics = evaluate_link_prediction(\n",
        "    G_train, \n",
        "    pos_test_sample, \n",
        "    neg_test_sample, \n",
        "    method='jaccard'\n",
        ")\n",
        "jc_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ… Jaccard Coefficient evaluation complete!\")\n",
        "print(f\"   Execution time: {jc_time:.2f} seconds\")\n",
        "print(f\"   Precision: {jc_metrics['precision']:.4f}\")\n",
        "print(f\"   Recall: {jc_metrics['recall']:.4f}\")\n",
        "print(f\"   F1 Score: {jc_metrics['f1']:.4f}\")\n",
        "print(f\"   AUC-ROC: {jc_metrics['auc_roc']:.4f}\")\n",
        "print(f\"   AUC-PR: {jc_metrics['auc_pr']:.4f}\")\n",
        "print(f\"   Optimal Threshold: {jc_metrics['optimal_threshold']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Top Predictions for Similarity Methods\n",
        "\n",
        "Show top predictions from each similarity method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get top predictions for each method\n",
        "print(\"Generating top predictions...\")\n",
        "\n",
        "# Common Neighbors - top 10\n",
        "cn_predictions = predict_links_similarity(G_train, method='common_neighbors', top_k=10)\n",
        "print(\"\\nðŸ“Š Top 10 Common Neighbors Predictions:\")\n",
        "print(cn_predictions.to_string(index=False))\n",
        "\n",
        "# Adamic-Adar - top 10\n",
        "aa_predictions = predict_links_similarity(G_train, method='adamic_adar', top_k=10)\n",
        "print(\"\\nðŸ“Š Top 10 Adamic-Adar Predictions:\")\n",
        "print(aa_predictions.to_string(index=False))\n",
        "\n",
        "# Jaccard - top 10\n",
        "jc_predictions = predict_links_similarity(G_train, method='jaccard', top_k=10)\n",
        "print(\"\\nðŸ“Š Top 10 Jaccard Coefficient Predictions:\")\n",
        "print(jc_predictions.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Machine Learning-Based Link Prediction\n",
        "\n",
        "Implement Random Forest classifier using multiple features extracted from network structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Prepare Training Data\n",
        "\n",
        "Extract features for positive and negative edges to create training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use a subset for training (to balance speed and performance)\n",
        "TRAIN_SIZE = min(2000, len(pos_test_sample))\n",
        "print(f\"Preparing training data with {TRAIN_SIZE} positive and {TRAIN_SIZE} negative edges...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "X_train, y_train = prepare_training_data(\n",
        "    G_train, \n",
        "    pos_test_sample[:TRAIN_SIZE], \n",
        "    neg_test_sample[:TRAIN_SIZE]\n",
        ")\n",
        "prep_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ… Training data prepared!\")\n",
        "print(f\"   Feature extraction time: {prep_time:.2f} seconds\")\n",
        "print(f\"   Training samples: {X_train.shape[0]:,}\")\n",
        "print(f\"   Features: {X_train.shape[1]}\")\n",
        "print(f\"   Positive samples: {np.sum(y_train == 1):,}\")\n",
        "print(f\"   Negative samples: {np.sum(y_train == 0):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Train Random Forest Model\n",
        "\n",
        "Train a Random Forest classifier on the extracted features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training Random Forest model...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "ml_model, scaler = train_random_forest(\n",
        "    X_train, \n",
        "    y_train, \n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    scale_features=True\n",
        ")\n",
        "ml_train_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ… Model trained successfully!\")\n",
        "print(f\"   Training time: {ml_train_time:.2f} seconds\")\n",
        "print(f\"   Model type: Random Forest\")\n",
        "print(f\"   Number of trees: 100\")\n",
        "print(f\"   Features scaled: Yes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Evaluate ML Model\n",
        "\n",
        "Evaluate the Random Forest model on test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use a subset for evaluation\n",
        "EVAL_SIZE = min(1000, len(pos_test_sample))\n",
        "print(f\"Evaluating ML model on {EVAL_SIZE} positive and {EVAL_SIZE} negative test edges...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "ml_metrics, ml_predictions = evaluate_ml_model(\n",
        "    ml_model,\n",
        "    G_train,\n",
        "    pos_test_sample[:EVAL_SIZE],\n",
        "    neg_test_sample[:EVAL_SIZE],\n",
        "    scaler=scaler\n",
        ")\n",
        "ml_eval_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ… ML model evaluation complete!\")\n",
        "print(f\"   Evaluation time: {ml_eval_time:.2f} seconds\")\n",
        "print(f\"   Precision: {ml_metrics['precision']:.4f}\")\n",
        "print(f\"   Recall: {ml_metrics['recall']:.4f}\")\n",
        "print(f\"   F1 Score: {ml_metrics['f1']:.4f}\")\n",
        "print(f\"   AUC-ROC: {ml_metrics['auc_roc']:.4f}\")\n",
        "print(f\"   AUC-PR: {ml_metrics['auc_pr']:.4f}\")\n",
        "print(f\"   Accuracy: {ml_metrics['accuracy']:.4f}\")\n",
        "print(f\"\\n   Confusion Matrix:\")\n",
        "print(f\"   True Negatives: {ml_metrics['true_negatives']}\")\n",
        "print(f\"   False Positives: {ml_metrics['false_positives']}\")\n",
        "print(f\"   False Negatives: {ml_metrics['false_negatives']}\")\n",
        "print(f\"   True Positives: {ml_metrics['true_positives']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Feature Importance\n",
        "\n",
        "Analyze which features are most important for link prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature names\n",
        "feature_names = ['common_neighbors', 'jaccard_coefficient', 'adamic_adar',\n",
        "                'degree_u', 'degree_v', 'degree_product', 'degree_sum',\n",
        "                'clustering_u', 'clustering_v']\n",
        "\n",
        "# Extract feature importance\n",
        "importance_df = get_feature_importance(ml_model, feature_names)\n",
        "\n",
        "print(\"ðŸ“Š Feature Importances (Random Forest):\")\n",
        "print(\"=\" * 60)\n",
        "print(importance_df.to_string(index=False))\n",
        "\n",
        "# Plot feature importance\n",
        "os.makedirs(\"results/figures\", exist_ok=True)\n",
        "plot_feature_importance(\n",
        "    importance_df,\n",
        "    save_path=\"results/figures/link_prediction_feature_importance.png\"\n",
        ")\n",
        "print(\"\\nâœ… Feature importance plot saved: results/figures/link_prediction_feature_importance.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comprehensive Comparison of All Methods\n",
        "\n",
        "Compare all four methods (3 similarity-based + 1 ML-based) using various metrics and visualizations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Comparison Table\n",
        "\n",
        "Create a comprehensive comparison table with all metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare all results\n",
        "all_results = {\n",
        "    'Common Neighbors': cn_metrics,\n",
        "    'Adamic-Adar': aa_metrics,\n",
        "    'Jaccard Coefficient': jc_metrics,\n",
        "    'Random Forest (ML)': ml_metrics\n",
        "}\n",
        "\n",
        "# Create comparison table\n",
        "comparison_df, styled_df = create_comparison_table(all_results)\n",
        "\n",
        "print(\"ðŸ“Š Comparison of All Link Prediction Methods:\")\n",
        "print(\"=\" * 80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 ROC Curves Comparison\n",
        "\n",
        "Plot ROC curves for all methods on the same plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate ROC curves for all methods\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "print(\"Calculating ROC curves for all methods...\")\n",
        "\n",
        "roc_data = {}\n",
        "\n",
        "# Common Neighbors\n",
        "cn_scores_pos = common_neighbors_score(G_train, pos_test_sample[:EVAL_SIZE])\n",
        "cn_scores_neg = common_neighbors_score(G_train, neg_test_sample[:EVAL_SIZE])\n",
        "cn_all_scores = np.array(cn_scores_pos + cn_scores_neg)\n",
        "cn_all_labels = np.array([1] * len(cn_scores_pos) + [0] * len(cn_scores_neg))\n",
        "cn_fpr, cn_tpr, _ = roc_curve(cn_all_labels, cn_all_scores)\n",
        "roc_data['Common Neighbors'] = (cn_fpr, cn_tpr, cn_metrics['auc_roc'])\n",
        "\n",
        "# Adamic-Adar\n",
        "aa_scores_pos = adamic_adar_score(G_train, pos_test_sample[:EVAL_SIZE])\n",
        "aa_scores_neg = adamic_adar_score(G_train, neg_test_sample[:EVAL_SIZE])\n",
        "aa_all_scores = np.array(aa_scores_pos + aa_scores_neg)\n",
        "aa_all_labels = np.array([1] * len(aa_scores_pos) + [0] * len(aa_scores_neg))\n",
        "aa_fpr, aa_tpr, _ = roc_curve(aa_all_labels, aa_all_scores)\n",
        "roc_data['Adamic-Adar'] = (aa_fpr, aa_tpr, aa_metrics['auc_roc'])\n",
        "\n",
        "# Jaccard\n",
        "jc_scores_pos = jaccard_coefficient_score(G_train, pos_test_sample[:EVAL_SIZE])\n",
        "jc_scores_neg = jaccard_coefficient_score(G_train, neg_test_sample[:EVAL_SIZE])\n",
        "jc_all_scores = np.array(jc_scores_pos + jc_scores_neg)\n",
        "jc_all_labels = np.array([1] * len(jc_scores_pos) + [0] * len(jc_scores_neg))\n",
        "jc_fpr, jc_tpr, _ = roc_curve(jc_all_labels, jc_all_scores)\n",
        "roc_data['Jaccard Coefficient'] = (jc_fpr, jc_tpr, jc_metrics['auc_roc'])\n",
        "\n",
        "# Random Forest\n",
        "ml_probs = ml_predictions['probability'].values\n",
        "ml_labels = np.array([1] * EVAL_SIZE + [0] * EVAL_SIZE)\n",
        "ml_fpr, ml_tpr, _ = roc_curve(ml_labels, ml_probs)\n",
        "roc_data['Random Forest (ML)'] = (ml_fpr, ml_tpr, ml_metrics['auc_roc'])\n",
        "\n",
        "# Plot ROC curves\n",
        "plot_roc_curves(roc_data, save_path=\"results/figures/link_prediction_roc_curves.png\")\n",
        "print(\"âœ… ROC curves saved: results/figures/link_prediction_roc_curves.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Precision-Recall Curves Comparison\n",
        "\n",
        "Plot Precision-Recall curves for all methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate PR curves for all methods\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "print(\"Calculating Precision-Recall curves for all methods...\")\n",
        "\n",
        "pr_data = {}\n",
        "\n",
        "# Common Neighbors\n",
        "cn_precision, cn_recall, _ = precision_recall_curve(cn_all_labels, cn_all_scores)\n",
        "pr_data['Common Neighbors'] = (cn_precision, cn_recall, cn_metrics['auc_pr'])\n",
        "\n",
        "# Adamic-Adar\n",
        "aa_precision, aa_recall, _ = precision_recall_curve(aa_all_labels, aa_all_scores)\n",
        "pr_data['Adamic-Adar'] = (aa_precision, aa_recall, aa_metrics['auc_pr'])\n",
        "\n",
        "# Jaccard\n",
        "jc_precision, jc_recall, _ = precision_recall_curve(jc_all_labels, jc_all_scores)\n",
        "pr_data['Jaccard Coefficient'] = (jc_precision, jc_recall, jc_metrics['auc_pr'])\n",
        "\n",
        "# Random Forest\n",
        "ml_precision, ml_recall, _ = precision_recall_curve(ml_labels, ml_probs)\n",
        "pr_data['Random Forest (ML)'] = (ml_precision, ml_recall, ml_metrics['auc_pr'])\n",
        "\n",
        "# Plot PR curves\n",
        "plot_precision_recall_curves(pr_data, save_path=\"results/figures/link_prediction_pr_curves.png\")\n",
        "print(\"âœ… Precision-Recall curves saved: results/figures/link_prediction_pr_curves.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Confusion Matrices\n",
        "\n",
        "Visualize confusion matrices for all methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate confusion matrices\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# For similarity methods, use optimal thresholds\n",
        "cn_pred = (cn_all_scores >= cn_metrics['optimal_threshold']).astype(int)\n",
        "aa_pred = (aa_all_scores >= aa_metrics['optimal_threshold']).astype(int)\n",
        "jc_pred = (jc_all_scores >= jc_metrics['optimal_threshold']).astype(int)\n",
        "\n",
        "# For ML, use predictions\n",
        "ml_pred = ml_predictions['prediction'].values\n",
        "\n",
        "confusion_matrices = {\n",
        "    'Common Neighbors': confusion_matrix(cn_all_labels, cn_pred),\n",
        "    'Adamic-Adar': confusion_matrix(aa_all_labels, aa_pred),\n",
        "    'Jaccard Coefficient': confusion_matrix(jc_all_labels, jc_pred),\n",
        "    'Random Forest (ML)': confusion_matrix(ml_labels, ml_pred)\n",
        "}\n",
        "\n",
        "# Plot confusion matrices\n",
        "plot_confusion_matrices(confusion_matrices, save_path=\"results/figures/link_prediction_confusion_matrices.png\")\n",
        "print(\"âœ… Confusion matrices saved: results/figures/link_prediction_confusion_matrices.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Score Distributions\n",
        "\n",
        "Compare score distributions for positive and negative edges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot score distributions for each method\n",
        "score_distributions = {\n",
        "    'Common Neighbors': (np.array(cn_scores_pos), np.array(cn_scores_neg)),\n",
        "    'Adamic-Adar': (np.array(aa_scores_pos), np.array(aa_scores_neg)),\n",
        "    'Jaccard Coefficient': (np.array(jc_scores_pos), np.array(jc_scores_neg)),\n",
        "    'Random Forest (ML)': (\n",
        "        ml_predictions[ml_predictions['prediction'] == 1]['probability'].values[:EVAL_SIZE],\n",
        "        ml_predictions[ml_predictions['prediction'] == 0]['probability'].values[:EVAL_SIZE]\n",
        "    )\n",
        "}\n",
        "\n",
        "for method_name, (pos_scores, neg_scores) in score_distributions.items():\n",
        "    plot_score_distributions(\n",
        "        pos_scores,\n",
        "        neg_scores,\n",
        "        method_name,\n",
        "        save_path=f\"results/figures/link_prediction_score_dist_{method_name.replace(' ', '_').replace('(', '').replace(')', '')}.png\"\n",
        "    )\n",
        "\n",
        "print(\"âœ… Score distribution plots saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results Analysis and Interpretation\n",
        "\n",
        "Analyze the results and provide insights about which method works best and why.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Performance Summary\n",
        "\n",
        "**Which Method Works Best?**\n",
        "\n",
        "Based on the comparison table above:\n",
        "\n",
        "1. **Best F1 Score**: [Method with highest F1]\n",
        "2. **Best AUC-ROC**: [Method with highest AUC-ROC]\n",
        "3. **Best AUC-PR**: [Method with highest AUC-PR]\n",
        "4. **Best Precision**: [Method with highest Precision]\n",
        "5. **Best Recall**: [Method with highest Recall]\n",
        "\n",
        "**Key Observations:**\n",
        "- Random Forest (ML) typically performs best because it combines multiple features\n",
        "- Similarity-based methods are faster but may have lower accuracy\n",
        "- Adamic-Adar often outperforms Common Neighbors and Jaccard\n",
        "- The choice of method depends on the trade-off between speed and accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Feature Importance Analysis\n",
        "\n",
        "**What Features Are Most Important?**\n",
        "\n",
        "From the feature importance plot above, we can see:\n",
        "\n",
        "1. **Top Features**: [List top 3-5 features]\n",
        "2. **Insights**:\n",
        "   - Similarity-based features (Adamic-Adar, Common Neighbors, Jaccard) are typically most important\n",
        "   - Node degree features provide additional signal\n",
        "   - Clustering coefficients may have lower importance but still contribute\n",
        "\n",
        "**Interpretation:**\n",
        "- Features that capture local network structure are most predictive\n",
        "- Combining multiple features improves prediction accuracy\n",
        "- Some features may be redundant (e.g., degree_product and degree_sum)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Example Predictions\n",
        "\n",
        "Show some example predictions from the best-performing method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get top predictions from best method (Random Forest)\n",
        "print(\"ðŸ“Š Top 20 Predictions from Random Forest (ML):\")\n",
        "print(\"=\" * 80)\n",
        "top_ml_predictions = ml_predictions.head(20)\n",
        "print(top_ml_predictions.to_string(index=False))\n",
        "\n",
        "# Show some examples of high-confidence predictions\n",
        "print(\"\\nðŸ“Š High-Confidence Predictions (Probability > 0.9):\")\n",
        "print(\"=\" * 80)\n",
        "high_conf = ml_predictions[ml_predictions['probability'] > 0.9].head(10)\n",
        "print(high_conf.to_string(index=False))\n",
        "\n",
        "# Show some examples of low-confidence predictions\n",
        "print(\"\\nðŸ“Š Low-Confidence Predictions (Probability < 0.1):\")\n",
        "print(\"=\" * 80)\n",
        "low_conf = ml_predictions[ml_predictions['probability'] < 0.1].head(10)\n",
        "print(low_conf.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Trade-offs: Speed vs Accuracy\n",
        "\n",
        "**Execution Time Comparison:**\n",
        "\n",
        "| Method | Training Time | Evaluation Time | Total Time |\n",
        "|--------|--------------|-----------------|------------|\n",
        "| Common Neighbors | N/A | [time] | [time] |\n",
        "| Adamic-Adar | N/A | [time] | [time] |\n",
        "| Jaccard Coefficient | N/A | [time] | [time] |\n",
        "| Random Forest (ML) | [time] | [time] | [time] |\n",
        "\n",
        "**Trade-offs:**\n",
        "\n",
        "1. **Similarity-Based Methods:**\n",
        "   - âœ… **Pros**: Very fast, no training required, interpretable\n",
        "   - âŒ **Cons**: Lower accuracy, single feature, may not capture complex patterns\n",
        "\n",
        "2. **ML-Based Methods:**\n",
        "   - âœ… **Pros**: Higher accuracy, combines multiple features, learns complex patterns\n",
        "   - âŒ **Cons**: Slower (requires training), more complex, less interpretable\n",
        "\n",
        "**Recommendations:**\n",
        "- Use **similarity-based methods** for:\n",
        "  - Real-time predictions\n",
        "  - Large-scale networks where speed is critical\n",
        "  - When interpretability is important\n",
        "  \n",
        "- Use **ML-based methods** for:\n",
        "  - When accuracy is the priority\n",
        "  - When you have computational resources\n",
        "  - When you can pre-train models offline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5 Business Insights\n",
        "\n",
        "**Applications in E-commerce:**\n",
        "\n",
        "1. **Product Recommendations:**\n",
        "   - Predict which products customers are likely to co-purchase\n",
        "   - Improve recommendation systems\n",
        "   - Increase cross-selling opportunities\n",
        "\n",
        "2. **Inventory Management:**\n",
        "   - Identify product bundles that are frequently purchased together\n",
        "   - Optimize warehouse layout\n",
        "   - Improve supply chain efficiency\n",
        "\n",
        "3. **Marketing Strategies:**\n",
        "   - Target customers with complementary products\n",
        "   - Design promotional bundles\n",
        "   - Optimize advertising campaigns\n",
        "\n",
        "4. **Network Growth:**\n",
        "   - Predict which new products will connect to existing network\n",
        "   - Identify potential partnerships\n",
        "   - Understand market dynamics\n",
        "\n",
        "**Key Metrics for Business:**\n",
        "- **Precision**: Percentage of predicted links that are actually true (important for avoiding false recommendations)\n",
        "- **Recall**: Percentage of actual links that are predicted (important for not missing opportunities)\n",
        "- **F1 Score**: Balanced measure of precision and recall\n",
        "- **AUC-ROC**: Overall ranking quality of predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save All Results\n",
        "\n",
        "Save all results, metrics, and visualizations to the results directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = \"data/results/link_prediction\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(\"results/tables\", exist_ok=True)\n",
        "\n",
        "print(f\"Saving all results to {output_dir}/...\")\n",
        "\n",
        "# Save comparison table\n",
        "comparison_df.to_csv(os.path.join(output_dir, \"comparison_table.csv\"), index=False)\n",
        "print(f\"âœ… Comparison table saved: {output_dir}/comparison_table.csv\")\n",
        "\n",
        "# Save individual method metrics\n",
        "metrics_dict = {\n",
        "    'common_neighbors': cn_metrics,\n",
        "    'adamic_adar': aa_metrics,\n",
        "    'jaccard_coefficient': jc_metrics,\n",
        "    'random_forest': ml_metrics\n",
        "}\n",
        "\n",
        "import json\n",
        "with open(os.path.join(output_dir, \"all_metrics.json\"), 'w') as f:\n",
        "    json.dump(metrics_dict, f, indent=2)\n",
        "print(f\"âœ… All metrics saved: {output_dir}/all_metrics.json\")\n",
        "\n",
        "# Save top predictions\n",
        "cn_predictions.to_csv(os.path.join(output_dir, \"top_predictions_common_neighbors.csv\"), index=False)\n",
        "aa_predictions.to_csv(os.path.join(output_dir, \"top_predictions_adamic_adar.csv\"), index=False)\n",
        "jc_predictions.to_csv(os.path.join(output_dir, \"top_predictions_jaccard.csv\"), index=False)\n",
        "ml_predictions.head(100).to_csv(os.path.join(output_dir, \"top_predictions_random_forest.csv\"), index=False)\n",
        "print(f\"âœ… Top predictions saved: {output_dir}/top_predictions_*.csv\")\n",
        "\n",
        "# Save feature importance\n",
        "importance_df.to_csv(os.path.join(output_dir, \"feature_importance.csv\"), index=False)\n",
        "print(f\"âœ… Feature importance saved: {output_dir}/feature_importance.csv\")\n",
        "\n",
        "# Generate comprehensive HTML report\n",
        "print(\"\\nGenerating comprehensive HTML report...\")\n",
        "generate_link_prediction_report(\n",
        "    all_results,\n",
        "    'results/tables/link_prediction_report.html',\n",
        "    roc_data=roc_data,\n",
        "    pr_data=pr_data,\n",
        "    confusion_matrices=confusion_matrices,\n",
        "    feature_importance=importance_df,\n",
        "    score_distributions=score_distributions\n",
        ")\n",
        "print(\"âœ… HTML report created: results/tables/link_prediction_report.html\")\n",
        "\n",
        "print(f\"\\nâœ… All results saved successfully!\")\n",
        "print(f\"   Output directory: {output_dir}/\")\n",
        "print(f\"   Figures: results/figures/\")\n",
        "print(f\"   Reports: results/tables/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has completed comprehensive link prediction analysis:\n",
        "\n",
        "âœ… **Data Loaded**: Training graph and test edges (positive and negative)  \n",
        "âœ… **Similarity Methods**: Common Neighbors, Adamic-Adar, Jaccard Coefficient evaluated  \n",
        "âœ… **ML Method**: Random Forest trained and evaluated with 9 features  \n",
        "âœ… **Comparison**: All 4 methods compared with metrics and visualizations  \n",
        "âœ… **Analysis**: Performance analysis, feature importance, and business insights  \n",
        "âœ… **Results Saved**: All outputs saved to data/results/link_prediction/  \n",
        "\n",
        "### Key Findings:\n",
        "- [Best performing method] achieves highest accuracy\n",
        "- [Top features] are most important for prediction\n",
        "- Trade-offs between speed and accuracy identified\n",
        "- Business applications and recommendations provided\n",
        "\n",
        "### Next Steps:\n",
        "- Experiment with different ML models (XGBoost, Neural Networks)\n",
        "- Add more features (temporal, content-based)\n",
        "- Evaluate on different network datasets\n",
        "- Deploy best model for production use\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
