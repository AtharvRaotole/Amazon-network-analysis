{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration: Amazon Product Co-Purchasing Network\n",
        "\n",
        "This notebook performs initial exploration and preprocessing of the Amazon Product Co-Purchasing Network dataset from SNAP.\n",
        "\n",
        "## Objectives:\n",
        "1. Load the dataset\n",
        "2. Clean and preprocess the graph\n",
        "3. Explore basic network properties\n",
        "4. Visualize degree distributions\n",
        "5. Prepare data for link prediction tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n",
        "\n",
        "Import all necessary libraries for data loading, preprocessing, analysis, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src directory to path for imports\n",
        "# Get the project root (parent of notebooks directory)\n",
        "# In Jupyter, cwd is typically the project root, but we handle both cases\n",
        "if Path.cwd().name == 'notebooks':\n",
        "    project_root = Path.cwd().parent\n",
        "else:\n",
        "    project_root = Path.cwd()\n",
        "\n",
        "sys.path.insert(0, str(project_root / 'src'))\n",
        "# Change to project root for data paths\n",
        "os.chdir(project_root)\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Network analysis\n",
        "import networkx as nx\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Project modules\n",
        "from data_loader import (\n",
        "    download_dataset, \n",
        "    load_graph, \n",
        "    load_communities,\n",
        "    save_graph\n",
        ")\n",
        "from preprocessing import (\n",
        "    remove_self_loops,\n",
        "    get_largest_component,\n",
        "    basic_statistics,\n",
        "    create_train_test_split,\n",
        "    save_splits\n",
        ")\n",
        "from exploratory_analysis import (\n",
        "    degree_distribution,\n",
        "    plot_degree_distribution,\n",
        "    compute_network_stats,\n",
        "    generate_statistics_report\n",
        ")\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid' if 'seaborn-v0_8-darkgrid' in plt.style.available \n",
        "              else 'seaborn-darkgrid' if 'seaborn-darkgrid' in plt.style.available \n",
        "              else 'ggplot')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset\n",
        "\n",
        "Download and load the Amazon co-purchasing network dataset. The dataset will be automatically downloaded if it doesn't exist in the `data/raw/` directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset (if not already present)\n",
        "print(\"Downloading dataset (if needed)...\")\n",
        "filepaths = download_dataset(data_dir=\"data/raw\")\n",
        "print(f\"Graph file: {filepaths['graph']}\")\n",
        "print(f\"Communities file: {filepaths['communities']}\")\n",
        "\n",
        "# Load the graph\n",
        "print(\"\\nLoading graph...\")\n",
        "G = load_graph(filepaths['graph'], is_gzipped=True)\n",
        "print(f\"Initial graph loaded: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
        "\n",
        "# Load ground-truth communities\n",
        "print(\"\\nLoading communities...\")\n",
        "communities = load_communities(filepaths['communities'], is_gzipped=True)\n",
        "print(f\"Loaded {len(communities):,} communities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing\n",
        "\n",
        "Clean the graph by removing self-loops and extracting the largest connected component. This ensures we work with a clean, connected network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Remove self-loops\n",
        "print(\"Removing self-loops...\")\n",
        "G_cleaned = remove_self_loops(G)\n",
        "print(f\"After removing self-loops: {G_cleaned.number_of_nodes():,} nodes, \"\n",
        "      f\"{G_cleaned.number_of_edges():,} edges\")\n",
        "\n",
        "# Step 2: Extract largest connected component\n",
        "print(\"\\nExtracting largest connected component...\")\n",
        "G_largest = get_largest_component(G_cleaned)\n",
        "print(f\"Largest component: {G_largest.number_of_nodes():,} nodes, \"\n",
        "      f\"{G_largest.number_of_edges():,} edges\")\n",
        "\n",
        "# Calculate percentage of nodes retained\n",
        "node_retention = (G_largest.number_of_nodes() / G.number_of_nodes()) * 100\n",
        "print(f\"\\nNode retention: {node_retention:.2f}% of original graph\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Basic Statistics\n",
        "\n",
        "Compute and display basic network statistics in a formatted table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute basic statistics\n",
        "print(\"Computing basic statistics...\")\n",
        "stats = basic_statistics(G_largest)\n",
        "\n",
        "# Display as formatted table\n",
        "stats_df = pd.DataFrame([stats]).T\n",
        "stats_df.columns = ['Value']\n",
        "stats_df.index.name = 'Metric'\n",
        "\n",
        "# Format the display\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BASIC NETWORK STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "print(stats_df.to_string())\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizations\n",
        "\n",
        "Create visualizations to understand the network structure, including degree distributions and network statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory for figures\n",
        "os.makedirs(\"results/figures\", exist_ok=True)\n",
        "\n",
        "# 5.1 Degree Distribution (Linear and Log-Log)\n",
        "print(\"Creating degree distribution plots...\")\n",
        "plot_degree_distribution(G_largest, save_path=\"results/figures/degree_distribution.png\")\n",
        "print(\"Saved: results/figures/degree_distribution.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.2 Degree Histogram with Statistics\n",
        "print(\"Creating degree histogram...\")\n",
        "degree_series = degree_distribution(G_largest)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.hist(degree_series.values, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax.set_xlabel('Degree', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Degree Distribution Histogram', fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add statistics text\n",
        "stats_text = f'Mean: {degree_series.mean():.2f}\\n'\n",
        "stats_text += f'Median: {degree_series.median():.2f}\\n'\n",
        "stats_text += f'Std: {degree_series.std():.2f}\\n'\n",
        "stats_text += f'Min: {degree_series.min()}\\n'\n",
        "stats_text += f'Max: {degree_series.max()}'\n",
        "ax.text(0.7, 0.95, stats_text, transform=ax.transAxes,\n",
        "        fontsize=11, verticalalignment='top',\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/figures/degree_histogram.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: results/figures/degree_histogram.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.3 Network Statistics Bar Chart\n",
        "print(\"Creating network statistics bar chart...\")\n",
        "# Compute comprehensive statistics\n",
        "network_stats = compute_network_stats(G_largest, sample_size=10000)\n",
        "\n",
        "# Select key statistics for visualization\n",
        "viz_stats = {\n",
        "    'Number of Nodes': network_stats['num_nodes'],\n",
        "    'Number of Edges': network_stats['num_edges'],\n",
        "    'Density (×10^6)': network_stats['density'] * 1e6,  # Scale for visibility\n",
        "    'Avg Degree': network_stats['avg_degree'],\n",
        "    'Avg Clustering': network_stats['avg_clustering'],\n",
        "    'Num Triangles (×10^3)': network_stats['num_triangles'] / 1000,  # Scale for visibility\n",
        "}\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "bars = ax.bar(viz_stats.keys(), viz_stats.values(), \n",
        "              color=['steelblue', 'coral', 'lightgreen', 'gold', 'plum', 'skyblue'],\n",
        "              edgecolor='black', alpha=0.7)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.2f}',\n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Value', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Network Statistics Summary', fontsize=14, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/figures/network_statistics.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: results/figures/network_statistics.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Cleaned Graph\n",
        "\n",
        "Save the preprocessed graph for future use to avoid reprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the cleaned graph\n",
        "print(\"Saving cleaned graph...\")\n",
        "cleaned_graph_path = \"data/processed/amazon_graph_cleaned.pkl\"\n",
        "save_graph(G_largest, cleaned_graph_path)\n",
        "print(f\"Cleaned graph saved to: {cleaned_graph_path}\")\n",
        "print(f\"Graph summary: {G_largest.number_of_nodes():,} nodes, \"\n",
        "      f\"{G_largest.number_of_edges():,} edges\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train/Test Split for Link Prediction\n",
        "\n",
        "Split the graph edges into training and test sets for link prediction tasks. This creates positive and negative examples for evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train/test split (80% train, 20% test)\n",
        "print(\"Creating train/test split for link prediction...\")\n",
        "G_train, positive_test_edges, negative_test_edges = create_train_test_split(\n",
        "    G_largest, \n",
        "    test_ratio=0.2, \n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"\\nSplit Summary:\")\n",
        "print(f\"  Training graph: {G_train.number_of_nodes():,} nodes, \"\n",
        "      f\"{G_train.number_of_edges():,} edges\")\n",
        "print(f\"  Positive test edges: {len(positive_test_edges):,}\")\n",
        "print(f\"  Negative test edges: {len(negative_test_edges):,}\")\n",
        "print(f\"  Test ratio: {len(positive_test_edges) / (len(positive_test_edges) + G_train.number_of_edges()):.2%}\")\n",
        "\n",
        "# Save the splits\n",
        "print(\"\\nSaving train/test splits...\")\n",
        "save_splits(G_train, positive_test_edges, negative_test_edges, \n",
        "            output_dir=\"data/processed/splits\")\n",
        "print(\"Splits saved to: data/processed/splits/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Ground-Truth Communities Summary\n",
        "\n",
        "Analyze and display summary statistics about the ground-truth communities in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze communities\n",
        "print(\"Analyzing ground-truth communities...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Basic community statistics\n",
        "community_sizes = [len(nodes) for nodes in communities.values()]\n",
        "community_sizes_series = pd.Series(community_sizes)\n",
        "\n",
        "print(f\"Total number of communities: {len(communities):,}\")\n",
        "print(f\"\\nCommunity Size Statistics:\")\n",
        "print(f\"  Minimum size: {min(community_sizes):,}\")\n",
        "print(f\"  Maximum size: {max(community_sizes):,}\")\n",
        "print(f\"  Mean size: {np.mean(community_sizes):.2f}\")\n",
        "print(f\"  Median size: {np.median(community_sizes):.2f}\")\n",
        "print(f\"  Standard deviation: {np.std(community_sizes):.2f}\")\n",
        "\n",
        "# Count nodes in communities\n",
        "total_nodes_in_communities = sum(community_sizes)\n",
        "unique_nodes_in_communities = len(set(node for nodes in communities.values() for node in nodes))\n",
        "print(f\"\\nNode Coverage:\")\n",
        "print(f\"  Total nodes in communities: {total_nodes_in_communities:,}\")\n",
        "print(f\"  Unique nodes in communities: {unique_nodes_in_communities:,}\")\n",
        "print(f\"  Coverage of graph nodes: {(unique_nodes_in_communities / G_largest.number_of_nodes() * 100):.2f}%\")\n",
        "\n",
        "# Display distribution of community sizes\n",
        "print(f\"\\nCommunity Size Distribution (top 10):\")\n",
        "print(community_sizes_series.value_counts().head(10).to_string())\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize community size distribution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Histogram of community sizes\n",
        "ax1.hist(community_sizes, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax1.set_xlabel('Community Size', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Community Size Distribution', fontsize=13, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Log-log plot\n",
        "community_size_counts = pd.Series(community_sizes).value_counts().sort_index()\n",
        "community_size_counts = community_size_counts[community_size_counts > 0]\n",
        "ax2.scatter(community_size_counts.index, community_size_counts.values,\n",
        "           alpha=0.6, s=50, color='coral', edgecolors='black', linewidth=0.5)\n",
        "ax2.set_xlabel('Community Size (log scale)', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Frequency (log scale)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Community Size Distribution (Log-Log)', fontsize=13, fontweight='bold')\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_yscale('log')\n",
        "ax2.grid(True, alpha=0.3, which='both')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results/figures/community_size_distribution.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Saved: results/figures/community_size_distribution.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has completed the initial data exploration:\n",
        "\n",
        "✅ **Dataset Loaded**: Amazon co-purchasing network with ground-truth communities  \n",
        "✅ **Preprocessed**: Removed self-loops and extracted largest component  \n",
        "✅ **Statistics Computed**: Basic and comprehensive network metrics  \n",
        "✅ **Visualizations Created**: Degree distributions and network statistics  \n",
        "✅ **Graph Saved**: Cleaned graph saved for future use  \n",
        "✅ **Splits Created**: Train/test split prepared for link prediction  \n",
        "✅ **Communities Analyzed**: Ground-truth community statistics computed  \n",
        "\n",
        "### Next Steps:\n",
        "- Use the cleaned graph for community detection algorithms\n",
        "- Train link prediction models using the train/test splits\n",
        "- Perform deeper network analysis and feature engineering\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
