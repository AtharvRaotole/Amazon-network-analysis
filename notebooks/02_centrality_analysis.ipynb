{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Centrality Analysis: Amazon Product Co-Purchasing Network\n",
        "\n",
        "This notebook performs comprehensive centrality analysis on the Amazon co-purchasing network to identify the most important/influential products.\n",
        "\n",
        "## Objectives:\n",
        "1. Load preprocessed graph\n",
        "2. Compute all centrality measures (PageRank, Degree, Betweenness, HITS)\n",
        "3. Compare execution times\n",
        "4. Analyze top nodes for each measure\n",
        "5. Visualize results and correlations\n",
        "6. Compute similarity metrics between measures\n",
        "7. Generate insights and interpretations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup\n",
        "\n",
        "Import all necessary libraries for centrality analysis and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src directory to path for imports\n",
        "# In Jupyter, cwd is typically the project root, but we handle both cases\n",
        "if Path.cwd().name == 'notebooks':\n",
        "    project_root = Path.cwd().parent\n",
        "else:\n",
        "    project_root = Path.cwd()\n",
        "\n",
        "sys.path.insert(0, str(project_root / 'src'))\n",
        "# Change to project root for data paths\n",
        "os.chdir(project_root)\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Network analysis\n",
        "import networkx as nx\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy.stats import pearsonr, kendalltau\n",
        "\n",
        "# Project modules\n",
        "from data_loader import load_saved_graph\n",
        "from centrality_analysis import (\n",
        "    compare_centrality_measures,\n",
        "    get_top_k_nodes,\n",
        "    save_centrality_results\n",
        ")\n",
        "from centrality_visualization import (\n",
        "    plot_top_k_comparison,\n",
        "    plot_correlation_heatmap,\n",
        "    plot_centrality_distributions,\n",
        "    plot_overlap_venn,\n",
        "    create_centrality_report\n",
        ")\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid' if 'seaborn-v0_8-darkgrid' in plt.style.available \n",
        "              else 'seaborn-darkgrid' if 'seaborn-darkgrid' in plt.style.available \n",
        "              else 'ggplot')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Preprocessed Graph\n",
        "\n",
        "Load the cleaned graph that was created during preprocessing. This graph has already been processed (self-loops removed, largest component extracted).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the preprocessed graph\n",
        "graph_path = \"data/processed/amazon_graph_cleaned.pkl\"\n",
        "\n",
        "print(f\"Loading graph from: {graph_path}\")\n",
        "G = load_saved_graph(graph_path)\n",
        "\n",
        "print(f\"\\nâœ… Graph loaded successfully!\")\n",
        "print(f\"   Nodes: {G.number_of_nodes():,}\")\n",
        "print(f\"   Edges: {G.number_of_edges():,}\")\n",
        "print(f\"   Type: {'Undirected' if not G.is_directed() else 'Directed'}\")\n",
        "print(f\"   Connected: {nx.is_connected(G) if not G.is_directed() else 'N/A (directed)'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compute All Centrality Measures\n",
        "\n",
        "Compute all four centrality measures with timing information. For large graphs, we use sampling for betweenness centrality to reduce computation time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute all centrality measures\n",
        "# For large graphs (~300K nodes), use k=1000 for betweenness sampling\n",
        "# This provides good approximation while keeping computation time reasonable\n",
        "\n",
        "print(\"Computing all centrality measures...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Set parameters\n",
        "k_top = 100  # Top-k nodes to extract\n",
        "betweenness_sample = 1000  # Sample size for betweenness (recommended for ~300K nodes)\n",
        "\n",
        "# Compute all measures with timing\n",
        "start_time = time.time()\n",
        "centrality_results = compare_centrality_measures(\n",
        "    G, \n",
        "    k=k_top, \n",
        "    betweenness_k=betweenness_sample\n",
        ")\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"âœ… All centrality measures computed in {total_time:.2f} seconds\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Execution Time Comparison\n",
        "\n",
        "Display a table comparing the execution time for each centrality measure. This helps understand the computational cost of each algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display execution time comparison\n",
        "if 'timing' in centrality_results:\n",
        "    timing_df = pd.DataFrame([\n",
        "        {'Measure': measure.capitalize(), 'Time (seconds)': time_taken}\n",
        "        for measure, time_taken in centrality_results['timing'].items()\n",
        "    ])\n",
        "    timing_df = timing_df.sort_values('Time (seconds)', ascending=False)\n",
        "    timing_df['Time (minutes)'] = timing_df['Time (seconds)'] / 60\n",
        "    timing_df['Percentage'] = (timing_df['Time (seconds)'] / timing_df['Time (seconds)'].sum() * 100).round(2)\n",
        "    \n",
        "    print(\"Execution Time Comparison\")\n",
        "    print(\"=\" * 60)\n",
        "    print(timing_df.to_string(index=False))\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nTotal computation time: {sum(centrality_results['timing'].values()):.2f} seconds\")\n",
        "    print(f\"Total computation time: {sum(centrality_results['timing'].values())/60:.2f} minutes\")\n",
        "else:\n",
        "    print(\"âš ï¸ Timing information not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Top-20 Nodes for Each Measure\n",
        "\n",
        "Display the top-20 most central nodes according to each centrality measure. This helps identify the most important/influential products in the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display top-20 nodes for each measure\n",
        "print(\"Top-20 Nodes by Centrality Measure\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# PageRank\n",
        "print(\"\\nðŸ“Š PageRank Top-20:\")\n",
        "print(\"-\" * 80)\n",
        "print(centrality_results['top_k_pagerank'].head(20).to_string(index=False))\n",
        "\n",
        "# Degree Centrality\n",
        "print(\"\\nðŸ“Š Degree Centrality Top-20:\")\n",
        "print(\"-\" * 80)\n",
        "print(centrality_results['top_k_degree'].head(20).to_string(index=False))\n",
        "\n",
        "# Betweenness Centrality\n",
        "print(\"\\nðŸ“Š Betweenness Centrality Top-20:\")\n",
        "print(\"-\" * 80)\n",
        "print(centrality_results['top_k_betweenness'].head(20).to_string(index=False))\n",
        "\n",
        "# HITS Hubs\n",
        "if 'top_k_hubs' in centrality_results:\n",
        "    print(\"\\nðŸ“Š HITS Hubs Top-20:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(centrality_results['top_k_hubs'].head(20).to_string(index=False))\n",
        "\n",
        "# HITS Authorities\n",
        "if 'top_k_authorities' in centrality_results:\n",
        "    print(\"\\nðŸ“Š HITS Authorities Top-20:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(centrality_results['top_k_authorities'].head(20).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualizations\n",
        "\n",
        "Create comprehensive visualizations to understand the centrality measures and their relationships.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Top-K Comparison Bar Charts\n",
        "\n",
        "Compare the top-20 nodes across all centrality measures using horizontal bar charts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory for figures\n",
        "os.makedirs(\"results/figures\", exist_ok=True)\n",
        "\n",
        "# Plot top-k comparison\n",
        "print(\"Creating top-k comparison visualization...\")\n",
        "plot_top_k_comparison(\n",
        "    centrality_results, \n",
        "    k=20, \n",
        "    save_path=\"results/figures/centrality_top_k_comparison.png\"\n",
        ")\n",
        "print(\"âœ… Saved: results/figures/centrality_top_k_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Correlation Heatmap\n",
        "\n",
        "Visualize the correlation between different centrality measures using a heatmap.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot correlation heatmap\n",
        "print(\"Creating correlation heatmap...\")\n",
        "plot_correlation_heatmap(\n",
        "    centrality_results['correlation_matrix'],\n",
        "    save_path=\"results/figures/centrality_correlation_heatmap.png\"\n",
        ")\n",
        "print(\"âœ… Saved: results/figures/centrality_correlation_heatmap.png\")\n",
        "\n",
        "# Display correlation matrix\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(\"=\" * 60)\n",
        "print(centrality_results['correlation_matrix'].round(3).to_string())\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Score Distributions\n",
        "\n",
        "Compare the distribution of centrality scores across different measures using violin plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot centrality distributions\n",
        "print(\"Creating distribution plots...\")\n",
        "plot_centrality_distributions(\n",
        "    centrality_results,\n",
        "    save_path=\"results/figures/centrality_distributions.png\",\n",
        "    plot_type='violin'\n",
        ")\n",
        "print(\"âœ… Saved: results/figures/centrality_distributions.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Overlap Analysis\n",
        "\n",
        "Visualize the overlap of top-k nodes across different centrality measures. This shows which nodes are consistently identified as important by multiple measures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot overlap visualization\n",
        "print(\"Creating overlap visualization...\")\n",
        "plot_overlap_venn(\n",
        "    centrality_results,\n",
        "    k=100,\n",
        "    save_path=\"results/figures/centrality_overlap_venn.png\"\n",
        ")\n",
        "print(\"âœ… Saved: results/figures/centrality_overlap_venn.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Statistical Analysis\n",
        "\n",
        "Compute detailed statistical measures to understand the relationships between centrality measures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Pearson Correlation\n",
        "\n",
        "Compute Pearson correlation coefficients between all pairs of centrality measures. This measures linear relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute pairwise Pearson correlations\n",
        "measures = ['pagerank', 'degree', 'betweenness']\n",
        "if 'hubs' in centrality_results:\n",
        "    measures.append('hubs')\n",
        "elif 'authorities' in centrality_results:\n",
        "    measures.append('authorities')\n",
        "\n",
        "# Get common nodes\n",
        "common_nodes = set(centrality_results[measures[0]].keys())\n",
        "for measure in measures[1:]:\n",
        "    common_nodes = common_nodes.intersection(set(centrality_results[measure].keys()))\n",
        "\n",
        "print(f\"Computing Pearson correlations for {len(common_nodes)} common nodes...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "pearson_results = []\n",
        "for i, measure1 in enumerate(measures):\n",
        "    for measure2 in measures[i+1:]:\n",
        "        scores1 = [centrality_results[measure1][node] for node in common_nodes]\n",
        "        scores2 = [centrality_results[measure2][node] for node in common_nodes]\n",
        "        corr, p_value = pearsonr(scores1, scores2)\n",
        "        pearson_results.append({\n",
        "            'Measure 1': measure1.capitalize(),\n",
        "            'Measure 2': measure2.capitalize(),\n",
        "            'Pearson r': corr,\n",
        "            'p-value': p_value\n",
        "        })\n",
        "\n",
        "pearson_df = pd.DataFrame(pearson_results)\n",
        "pearson_df = pearson_df.sort_values('Pearson r', ascending=False)\n",
        "\n",
        "print(pearson_df.to_string(index=False))\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Kendall Tau Correlation\n",
        "\n",
        "Compute Kendall Tau rank correlation coefficients. This measures monotonic relationships and is more robust to outliers than Pearson correlation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute pairwise Kendall Tau correlations\n",
        "print(f\"Computing Kendall Tau correlations for {len(common_nodes)} common nodes...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "kendall_results = []\n",
        "for i, measure1 in enumerate(measures):\n",
        "    for measure2 in measures[i+1:]:\n",
        "        scores1 = [centrality_results[measure1][node] for node in common_nodes]\n",
        "        scores2 = [centrality_results[measure2][node] for node in common_nodes]\n",
        "        tau, p_value = kendalltau(scores1, scores2)\n",
        "        kendall_results.append({\n",
        "            'Measure 1': measure1.capitalize(),\n",
        "            'Measure 2': measure2.capitalize(),\n",
        "            'Kendall Ï„': tau,\n",
        "            'p-value': p_value\n",
        "        })\n",
        "\n",
        "kendall_df = pd.DataFrame(kendall_results)\n",
        "kendall_df = kendall_df.sort_values('Kendall Ï„', ascending=False)\n",
        "\n",
        "print(kendall_df.to_string(index=False))\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Jaccard Similarity of Top-100 Sets\n",
        "\n",
        "Compute Jaccard similarity between the top-100 node sets for each pair of centrality measures. This shows how much overlap there is in the most important nodes identified by different measures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Jaccard similarity for top-100 sets\n",
        "def jaccard_similarity(set1, set2):\n",
        "    \"\"\"Compute Jaccard similarity between two sets.\"\"\"\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "# Get top-100 node sets\n",
        "top_k = 100\n",
        "top_sets = {}\n",
        "top_sets['pagerank'] = set(centrality_results['top_k_pagerank'].head(top_k)['node_id'].values)\n",
        "top_sets['degree'] = set(centrality_results['top_k_degree'].head(top_k)['node_id'].values)\n",
        "top_sets['betweenness'] = set(centrality_results['top_k_betweenness'].head(top_k)['node_id'].values)\n",
        "if 'top_k_hubs' in centrality_results:\n",
        "    top_sets['hubs'] = set(centrality_results['top_k_hubs'].head(top_k)['node_id'].values)\n",
        "if 'top_k_authorities' in centrality_results:\n",
        "    top_sets['authorities'] = set(centrality_results['top_k_authorities'].head(top_k)['node_id'].values)\n",
        "\n",
        "print(f\"Computing Jaccard similarity for top-{top_k} node sets...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "jaccard_results = []\n",
        "for i, (name1, set1) in enumerate(top_sets.items()):\n",
        "    for name2, set2 in list(top_sets.items())[i+1:]:\n",
        "        jaccard = jaccard_similarity(set1, set2)\n",
        "        intersection = len(set1 & set2)\n",
        "        jaccard_results.append({\n",
        "            'Measure 1': name1.capitalize(),\n",
        "            'Measure 2': name2.capitalize(),\n",
        "            'Jaccard Similarity': jaccard,\n",
        "            'Intersection Size': intersection,\n",
        "            'Union Size': len(set1 | set2)\n",
        "        })\n",
        "\n",
        "jaccard_df = pd.DataFrame(jaccard_results)\n",
        "jaccard_df = jaccard_df.sort_values('Jaccard Similarity', ascending=False)\n",
        "\n",
        "print(jaccard_df.to_string(index=False))\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save All Results\n",
        "\n",
        "Save all centrality results, visualizations, and analysis outputs to the results directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "output_dir = \"data/results/centrality\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(\"results/tables\", exist_ok=True)\n",
        "\n",
        "print(f\"Saving centrality results to {output_dir}/...\")\n",
        "\n",
        "# Save centrality results (CSV files)\n",
        "save_centrality_results(centrality_results, output_dir=\"results/tables\")\n",
        "print(\"âœ… Centrality scores and top-k nodes saved to results/tables/\")\n",
        "\n",
        "# Save correlation matrices\n",
        "correlation_path = os.path.join(output_dir, \"correlation_analysis.csv\")\n",
        "correlation_data = {\n",
        "    'pearson': pearson_df,\n",
        "    'kendall': kendall_df,\n",
        "    'jaccard': jaccard_df\n",
        "}\n",
        "\n",
        "# Save each correlation analysis\n",
        "pearson_df.to_csv(os.path.join(output_dir, \"pearson_correlation.csv\"), index=False)\n",
        "kendall_df.to_csv(os.path.join(output_dir, \"kendall_tau_correlation.csv\"), index=False)\n",
        "jaccard_df.to_csv(os.path.join(output_dir, \"jaccard_similarity.csv\"), index=False)\n",
        "\n",
        "print(f\"âœ… Correlation analyses saved to {output_dir}/\")\n",
        "\n",
        "# Create comprehensive HTML report\n",
        "print(\"\\nCreating comprehensive HTML report...\")\n",
        "create_centrality_report(centrality_results, output_path=\"results/tables/centrality_report.html\")\n",
        "print(\"âœ… HTML report created: results/tables/centrality_report.html\")\n",
        "\n",
        "print(f\"\\nâœ… All results saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Insights and Interpretation\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Execution Time Analysis:**\n",
        "   - Betweenness centrality is typically the slowest due to shortest path computations\n",
        "   - Degree centrality is the fastest (O(n) complexity)\n",
        "   - PageRank and HITS have similar computational costs\n",
        "\n",
        "2. **Top Nodes:**\n",
        "   - Different measures identify different \"important\" nodes\n",
        "   - High degree nodes are not always high in other measures\n",
        "   - Some nodes consistently appear in top-k across multiple measures\n",
        "\n",
        "3. **Correlation Analysis:**\n",
        "   - **High correlation** (r > 0.7): Measures that identify similar nodes\n",
        "   - **Moderate correlation** (0.3 < r < 0.7): Partial agreement\n",
        "   - **Low correlation** (r < 0.3): Measures capture different aspects of importance\n",
        "\n",
        "4. **Jaccard Similarity:**\n",
        "   - Shows overlap in top-k sets\n",
        "   - High similarity (>0.5): Measures agree on important nodes\n",
        "   - Low similarity (<0.3): Measures capture different types of importance\n",
        "\n",
        "5. **Distribution Analysis:**\n",
        "   - Most nodes have low centrality scores (power-law distribution)\n",
        "   - A few nodes have very high centrality (hubs/influencers)\n",
        "   - Different measures have different score distributions\n",
        "\n",
        "### Interpretation:\n",
        "\n",
        "- **PageRank**: Identifies nodes that are well-connected to other well-connected nodes (recursive importance)\n",
        "- **Degree Centrality**: Simple measure of direct connections (local importance)\n",
        "- **Betweenness Centrality**: Identifies nodes that act as bridges (structural importance)\n",
        "- **HITS**: Separates hubs (point to authorities) and authorities (pointed to by hubs)\n",
        "\n",
        "### Business Implications:\n",
        "\n",
        "For the Amazon co-purchasing network:\n",
        "- Products with high PageRank are frequently co-purchased with other popular products\n",
        "- High degree products are co-purchased with many different products\n",
        "- High betweenness products connect different product categories\n",
        "- Understanding these patterns can inform:\n",
        "  - Product recommendations\n",
        "  - Marketing strategies\n",
        "  - Inventory management\n",
        "  - Cross-selling opportunities\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
